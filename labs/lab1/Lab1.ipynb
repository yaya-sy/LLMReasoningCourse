{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwm9txhnWAFzX1pYha3X6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/LLMReasoningCourse/blob/main/labs/lab1/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Improving the output quality of LLMs with better decoding algorithms"
      ],
      "metadata": {
        "id": "eiH3zz4aiZ-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see how to improve LLMs for French to English translation through better sampling algorithms and hyper-parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "22k_u2MXUsU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "TCShFRdpiDm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone this github repo: https://github.com/yaya-sy/LLMReasoningCourse.git\n",
        "\n",
        "In the folder `labs/lab1/data`, you will find a parallel corpus for English and French, meaning the $i^{th}$ line of the file `fr.txt` is the french translation of the $i^{th}$ in the file `en.txt`"
      ],
      "metadata": {
        "id": "eRqMmPew9W2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yaya-sy/LLMReasoningCourse.git"
      ],
      "metadata": {
        "id": "66ne-u6EkNa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `load_data` that return a data of this format:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"fr\": {\"dev\": [french development corpus], \"test\": [french test corpus],\n",
        "  \"en\": {\"dev\": [english development corpus], \"test\": [englih test corpus],\n",
        "}\n",
        "```\n",
        "\n",
        "Use 30% of the data for dev and the remain for test. Shuffle the pairs of translations, but the data must remain aligned."
      ],
      "metadata": {
        "id": "DkiEzgKjiINE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def load_data(path):\n",
        "    data = {\"fr\": {\"dev\": [], \"test\": []}, \"en\": {\"dev\": [], \"test\": []}}\n",
        "    return data\n",
        "data = load_data(\"LLMReasoningCourse/labs/lab1/data\")"
      ],
      "metadata": {
        "id": "Q98H00mwjo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are we doing this? What is the difference between dev and test corpus? Which split should we use to tune our algorithms?"
      ],
      "metadata": {
        "id": "KYWKeyi1jucj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "6Df9tYQYlrXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use `HuggingFaceTB/SmolLM2-135M-Instruct` for this lab."
      ],
      "metadata": {
        "id": "iXV9KKbxmV92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and the tokenizer. Load the model on the GPU if available\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "device = \"\"\n",
        "model = \"\"\n",
        "tokenizer = \"\"\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "mT5az6cJlto_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What arguments in the model loader can we use to reduce the memory footprint of the model?"
      ],
      "metadata": {
        "id": "FogLWkW0miL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation loop"
      ],
      "metadata": {
        "id": "aylcDz2em7YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class if the main class we use for generating text from the LLM. You will implement the missing parts."
      ],
      "metadata": {
        "id": "hwGdyM22nATN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbB4GkehUq0Q"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Optional\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.w = self.model.lm_head.weight.data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eos = self.tokenizer.eos_token_id\n",
        "        self.prompt = \"You are given a French text, provide faithfull translation to English. English text:\\n\\n\"\n",
        "\n",
        "    def tokenize(self, texts: List[str]):\n",
        "        \"\"\"Tokenize the texts\"\"\"\n",
        "        conversations = [\n",
        "            [{\"role\": \"user\", \"content\": f'{self.prompt} {text}'}]\n",
        "            for text in texts]\n",
        "        # TODO: call apply_chat_template from the tokenizer class with the right arguments to output torch tensors of the token ids\n",
        "        # Which padding side to use? why?\n",
        "\n",
        "        pass\n",
        "\n",
        "    def decode(self, generated_token_ids):\n",
        "        decoded = []\n",
        "        where_eos_is_reached = generated_token_ids == self.eos\n",
        "        for idx, batch in enumerate(where_eos_is_reached):\n",
        "            if batch.any(): # if eos id is present for each sequence in the batch\n",
        "                eos_idx = batch.int().argmax()\n",
        "                decoded.append(generated_token_ids[idx, :eos_idx].tolist())\n",
        "            else: # else just return the uncompleted generation\n",
        "                decoded.append(generated_token_ids[idx].tolist())\n",
        "        return self.tokenizer.batch_decode(decoded, skip_special_tokens=True)\n",
        "\n",
        "    def logits(self, h):\n",
        "        \"\"\"Compute the logits\"\"\"\n",
        "        pass\n",
        "\n",
        "    def softmax(self, logits):\n",
        "        # see: https://stackoverflow.com/questions/42599498/numerically-stable-softmax\n",
        "        \"\"\"Normalizes the logits to have probabilities\"\"\"\n",
        "        pass\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,\n",
        "                 texts: List[str],\n",
        "                 temperature: Optional[int]=None,\n",
        "                 top_k: Optional[int]=None,\n",
        "                 top_p: Optional[float]=None,\n",
        "                 max_new_tokens: int=16):\n",
        "\n",
        "        batch_size = len(texts)\n",
        "        tokenized = self.tokenize(texts)\n",
        "        token_ids = tokenized.to(self.model.device) # contains {input_ids: ..., attention_mask: ...}\n",
        "\n",
        "        generated = torch.tensor([], dtype=torch.long, device=self.model.device) # will contain the generated token id\n",
        "        while generated.numel() == 0 or generated.shape[-1] < max_new_tokens: # generate until the max_new_tokens is reached\n",
        "            outputs = self.model(**token_ids, output_hidden_states=True)\n",
        "            h = outputs.hidden_states[-1] # last hidden state\n",
        "            logits = self.logits(h)\n",
        "            logits = outputs.logits[:, -1, :].float()\n",
        "            vocab_size = logits.shape[-1]\n",
        "            if temperature is not None and temperature > 0:\n",
        "                # apply temperature\n",
        "                pass\n",
        "\n",
        "            if top_k is not None:\n",
        "                # filter top_k logits by setting the ignored logits to float(\"-Inf\")\n",
        "                pass\n",
        "\n",
        "            if top_p is not None:\n",
        "                # filter the logits for top_p by setting float('-Inf') to the token logits that don't reach the top_p\n",
        "\n",
        "            probabilities = self.softmax(logits)\n",
        "            next_tokens = torch.multinomial(probabilities, 1).long()\n",
        "            token_ids[\"input_ids\"] = torch.cat((token_ids[\"input_ids\"], next_tokens), dim=-1)\n",
        "            token_ids[\"attention_mask\"] = torch.cat((token_ids[\"attention_mask\"], torch.ones_like(next_tokens)), dim=-1)\n",
        "            generated = torch.cat((generated, next_tokens), dim=-1)\n",
        "\n",
        "        # decode the generations\n",
        "        decoded = self.decode(generated)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement and test the generation class"
      ],
      "metadata": {
        "id": "UWTPWbwaDK4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(model=model,\n",
        "                      tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "quWBA0FixQjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate(texts=[\"J'adore le chocolat.\", \"Ce film est vraiment magnifique !\"])"
      ],
      "metadata": {
        "id": "JQPwRN7gzZua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate(texts=[\"J'adore le chocolat.\", \"Ce film est vraiment magnifique !\"], temperature=0.4, top_p=0.7, max_new_tokens=16)"
      ],
      "metadata": {
        "id": "qfPfmr5y8pi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using batch generation here. Is it faster than single-batch generation. What are the limits of batch generation?\n",
        "\n",
        "What are the solutions to improve the generation speed? see: https://huggingface.co/blog/continuous_batching"
      ],
      "metadata": {
        "id": "8Q7wfc7kon73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def translate_corpus(corpus: List[str], batch_size: int=16, t=0.2):\n",
        "    data = sorted(enumerate(corpus), key=lambda x: generator.tokenize([x[1]]).input_ids.shape[-1])\n",
        "\n",
        "    translated = [None] * len(corpus)\n",
        "\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        indices, texts = zip(*data[i : i + batch_size])\n",
        "\n",
        "        preds = generator.generate(list(texts), temperature=t, top_p=0.8, max_new_tokens=64)\n",
        "\n",
        "        # Place predictions back in original slots\n",
        "        for idx, pred in zip(indices, preds):\n",
        "            translated[idx] = pred\n",
        "\n",
        "    return translated"
      ],
      "metadata": {
        "id": "75DcAAwVoi6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate the french dev corpus to English\n",
        "translated = translate_corpus(data[\"fr\"][\"dev\"])"
      ],
      "metadata": {
        "id": "aZ3MPjQbDRP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(data[\"fr\"][\"dev\"][:10], data[\"en\"][\"dev\"][:10]))"
      ],
      "metadata": {
        "id": "qcgOR0_PDbz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(data[\"fr\"][\"dev\"][:10], translated[:10]))"
      ],
      "metadata": {
        "id": "tHaVH3-HGd44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transalte the french dev corpus to english at different temperatures: `[0.1, 0.4, 1.0, 1.5, 2.0, 4.0, 8.0]` and plot the log-likelihood of the french translations given the english texts. A recall of the definition of the log-likehood:\n",
        "\n",
        "$$\n",
        "\\log \\mathcal{L}(\\mathcal{D}, \\theta) = \\frac{1}{N} \\sum\\limits_{(S, T) \\in \\mathcal{D}} \\sum\\limits_{i=1}^{|S|} \\log p(T_{i}|T_{<i}, S; \\theta)\n",
        "$$\n",
        "\n",
        "where S is the source sentence (the English sentence) and T the french translated sentence."
      ],
      "metadata": {
        "id": "dDEdlhAVq2l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_likelihood(corpus: List[str], batch_size: int=16):\n",
        "    pass"
      ],
      "metadata": {
        "id": "K-GuPIDPoI5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use sacre bleu to evaluate the translations on BLEU and CHRF metrics: https://github.com/mjpost/sacrebleu"
      ],
      "metadata": {
        "id": "0qXNx_4PKBtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}