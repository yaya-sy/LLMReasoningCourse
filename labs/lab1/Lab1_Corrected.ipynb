{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDol7SuOX5sXy9saBQdr0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/LLMReasoningCourse/blob/main/labs/lab1/Lab1_Corrected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1: Improving the output quality of LLMs with better decoding algorithms"
      ],
      "metadata": {
        "id": "eiH3zz4aiZ-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see how to improve LLMs for French to English translation through better sampling algorithms and hyper-parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "22k_u2MXUsU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "TCShFRdpiDm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone this github repo: https://github.com/yaya-sy/LLMReasoningCourse.git\n",
        "\n",
        "In the folder `labs/lab1/data`, you will find a parallel corpus for English and French, meaning the $i^{th}$ line of the file `fr.txt` is the french translation of the $i^{th}$ in the file `en.txt`"
      ],
      "metadata": {
        "id": "eRqMmPew9W2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yaya-sy/LLMReasoningCourse.git"
      ],
      "metadata": {
        "id": "66ne-u6EkNa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `load_data` that return a data of this format:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"fr\": {\"dev\": [french development corpus], \"test\": [french test corpus],\n",
        "  \"en\": {\"dev\": [english development corpus], \"test\": [englih test corpus],\n",
        "}\n",
        "```\n",
        "\n",
        "Use 30% of the data for dev and the remain for test. Shuffle the pairs of translations, but the data must remain aligned."
      ],
      "metadata": {
        "id": "DkiEzgKjiINE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def load_data(path):\n",
        "    with open(f\"{path}/fr.txt\", \"r\") as f:\n",
        "        fr = f.readlines()\n",
        "    with open(f\"{path}/en.txt\", \"r\") as f:\n",
        "        en = f.readlines()\n",
        "\n",
        "    en_fr = list(zip(en, fr))\n",
        "    random.shuffle(en_fr)\n",
        "    split_idx = int(len(en_fr) * 0.3)\n",
        "    dev_data = en_fr[:split_idx]\n",
        "    test_data = en_fr[split_idx:]\n",
        "\n",
        "    data = {\"fr\": {\"dev\": [], \"test\": []}, \"en\": {\"dev\": [], \"test\": []}}\n",
        "    for en, fr in dev_data:\n",
        "        data[\"fr\"][\"dev\"].append(fr.strip())\n",
        "        data[\"en\"][\"dev\"].append(en.strip())\n",
        "    for en, fr in test_data:\n",
        "        data[\"fr\"][\"test\"].append(fr.strip())\n",
        "        data[\"en\"][\"test\"].append(en.strip())\n",
        "\n",
        "    return data\n",
        "data = load_data(\"LLMReasoningCourse/labs/lab1/data\")"
      ],
      "metadata": {
        "id": "Q98H00mwjo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are we doing this? What is the difference between dev and test corpus? Which split should we use to tune our algorithms?"
      ],
      "metadata": {
        "id": "KYWKeyi1jucj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "6Df9tYQYlrXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use `HuggingFaceTB/SmolLM2-135M-Instruct` for this lab."
      ],
      "metadata": {
        "id": "iXV9KKbxmV92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and the tokenizer. Load the model on the GPU if available\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\", dtype=torch.bfloat16, token=\"\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "mT5az6cJlto_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What arguments in the model loader can we use to reduce the memory footprint of the model?"
      ],
      "metadata": {
        "id": "FogLWkW0miL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation loop"
      ],
      "metadata": {
        "id": "aylcDz2em7YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class if the main class we use for generating text from the LLM. You will implement the missing parts."
      ],
      "metadata": {
        "id": "hwGdyM22nATN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbB4GkehUq0Q"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union, Optional\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.w = self.model.lm_head.weight.data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eos = self.tokenizer.eos_token_id\n",
        "\n",
        "    def tokenize(self, texts: List[str]):\n",
        "        \"\"\"Tokenize the texts\"\"\"\n",
        "        conversations = [\n",
        "            [{\"role\": \"user\",\n",
        "              \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\nThe French text: {text}\\n\\nYour English translation:\"}]\n",
        "            for text in texts]\n",
        "        # TODO: call apply_chat_template from the tokenizer class with the right arguments to output torch tensors of the token ids\n",
        "        # Which padding side to use? why?\n",
        "        templated = self.tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=True)\n",
        "        tokenized = self.tokenizer(templated, padding_side=\"left\", padding=True, return_tensors=\"pt\")\n",
        "        return tokenized\n",
        "\n",
        "    def decode(self, generated_token_ids):\n",
        "        decoded = []\n",
        "        where_eos_is_reached = generated_token_ids == self.eos\n",
        "        for idx, batch in enumerate(where_eos_is_reached):\n",
        "            if batch.any(): # if eos id is present for each sequence in the batch\n",
        "                eos_idx = batch.int().argmax()\n",
        "                decoded.append(generated_token_ids[idx, :eos_idx].tolist())\n",
        "            else: # else just return the uncompleted generation\n",
        "                decoded.append(generated_token_ids[idx].tolist())\n",
        "        return self.tokenizer.batch_decode(decoded, skip_special_tokens=True)\n",
        "\n",
        "    def logits(self, c):\n",
        "        return c @ self.w.T\n",
        "\n",
        "    def softmax(self, logits):\n",
        "        # see: https://stackoverflow.com/questions/42599498/numerically-stable-softmax\n",
        "        logits -= logits.max(dim=-1, keepdim=True).values\n",
        "        scores = logits.exp()\n",
        "        return scores / scores.sum(dim=-1, keepdim=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,\n",
        "                 texts: List[str],\n",
        "                 temperature: Optional[int]=None,\n",
        "                 top_k: Optional[int]=None,\n",
        "                 top_p: Optional[float]=None,\n",
        "                 max_new_tokens: int=16):\n",
        "\n",
        "        batch_size = len(texts)\n",
        "        tokenized = self.tokenize(texts)\n",
        "        token_ids = tokenized.to(self.model.device) # contains {input_ids: ..., attention_mask: ...}\n",
        "\n",
        "        generated = torch.tensor([], dtype=torch.long, device=self.model.device) # will contain the generated token id\n",
        "        while generated.numel() == 0 or generated.shape[-1] < max_new_tokens: # generate until the max_new_tokens is reached\n",
        "            outputs = self.model(**token_ids, output_hidden_states=True)\n",
        "            h = outputs.hidden_states[-1] # last hidden state\n",
        "            logits = self.logits(h)\n",
        "            logits = outputs.logits[:, -1, :].float()\n",
        "            vocab_size = logits.shape[-1]\n",
        "            if temperature is not None and temperature > 0:\n",
        "                logits /= temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                top_k_logits = torch.topk(logits, k=vocab_size-top_k, largest=False, dim=-1).indices\n",
        "                logits.scatter_(-1, top_k_logits, float('-inf'))\n",
        "\n",
        "            if top_p is not None:\n",
        "                probabilities = self.softmax(logits)\n",
        "                sorted_probabilities, sorted_indices = torch.sort(probabilities, descending=True, dim=-1)\n",
        "                cumulative_probabilities = torch.cumsum(sorted_probabilities, dim=-1)\n",
        "\n",
        "                # Remove tokens where cumsum < top_p\n",
        "                top_p_thresold = cumulative_probabilities < top_p\n",
        "\n",
        "                indices_to_remove = torch.zeros_like(logits, dtype=torch.bool)\n",
        "                # fill True to kept indices (indices that reached the top_p thresold)\n",
        "                indices_to_remove.scatter_(-1, sorted_indices, top_p_thresold)\n",
        "\n",
        "                # Apply mask to logits\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            probabilities = self.softmax(logits)\n",
        "            next_tokens = torch.multinomial(probabilities, 1).long()\n",
        "            token_ids[\"input_ids\"] = torch.cat((token_ids[\"input_ids\"], next_tokens), dim=-1)\n",
        "            token_ids[\"attention_mask\"] = torch.cat((token_ids[\"attention_mask\"], torch.ones_like(next_tokens)), dim=-1)\n",
        "            generated = torch.cat((generated, next_tokens), dim=-1)\n",
        "\n",
        "        # decode the generations\n",
        "        decoded = self.decode(generated)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement and test the generation class"
      ],
      "metadata": {
        "id": "UWTPWbwaDK4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(model=model,\n",
        "                      tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "quWBA0FixQjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate(texts=[\"J'adore le chocolat.\", \"Ce film est vraiment magnifique !\"], top_k=1)"
      ],
      "metadata": {
        "id": "JQPwRN7gzZua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.generate(texts=[\"J'adore le chocolat.\", \"Ce film est vraiment magnifique !\"], temperature=0.4, top_p=0.7, max_new_tokens=16)"
      ],
      "metadata": {
        "id": "qfPfmr5y8pi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using batch generation here. Is it faster than single-batch generation. What are the limits of batch generation?\n",
        "\n",
        "Batched generation is costly in memory. Moreover, the model continues to forward some sequences even if their generations have finished. To improve this we have to implement an eviction policy: remove from the batch the sequences when that have reached `end_of_sentence_token` and add new sequences to the batch.\n",
        "  "
      ],
      "metadata": {
        "id": "8Q7wfc7kon73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "def translate_corpus(corpus: List[str], batch_size: int=8, temperature=0.2):\n",
        "    data = sorted(enumerate(corpus), key=lambda x: generator.tokenize([x[1]]).input_ids.shape[-1])\n",
        "\n",
        "    translated = [None] * len(corpus)\n",
        "\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        indices, texts = zip(*data[i : i + batch_size])\n",
        "\n",
        "        preds = generator.generate(list(texts), temperature=temperature, top_p=0.4, max_new_tokens=64)\n",
        "\n",
        "        # Place predictions back in original slots\n",
        "        for idx, pred in zip(indices, preds):\n",
        "            translated[idx] = pred\n",
        "\n",
        "    return translated"
      ],
      "metadata": {
        "id": "75DcAAwVoi6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate the french dev corpus to English\n",
        "# translated = translate_corpus(data[\"fr\"][\"dev\"])"
      ],
      "metadata": {
        "id": "aZ3MPjQbDRP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(data[\"fr\"][\"dev\"][:10], data[\"en\"][\"dev\"][:10]))"
      ],
      "metadata": {
        "id": "qcgOR0_PDbz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(data[\"fr\"][\"dev\"][:10], translated[:10]))"
      ],
      "metadata": {
        "id": "tHaVH3-HGd44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transalte the french dev corpus to english at different temperatures: `[0.1, 0.4, 1.0, 1.5, 2.0, 4.0, 8.0]` and plot the log-likelihood of the English translations given the French texts. A recall of the definition of the log-likehood:\n",
        "\n",
        "$$\n",
        "\\log \\mathcal{L}(\\mathcal{D}, \\theta) = \\frac{1}{N} \\sum\\limits_{(S, T) \\in \\mathcal{D}} \\sum\\limits_{i=1}^{|T|} \\log p(T_{i}|T_{<i}, S; \\theta)\n",
        "$$\n",
        "\n",
        "where S is the source sentence (the English sentence) and T the french translated sentence."
      ],
      "metadata": {
        "id": "dDEdlhAVq2l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature_translated = {t: translate_corpus(data[\"fr\"][\"dev\"], temperature=t) for t in [0.1, 0.4, 1.0, 1.5, 2.0, 4.0, 8.0]}"
      ],
      "metadata": {
        "id": "DXd3b2fhN7yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "from typing import Tuple\n",
        "\n",
        "@torch.no_grad()\n",
        "def log_likelihood(texts: List[Tuple[str, str]], batch_size: int=16):\n",
        "    loglikelihoods = []\n",
        "\n",
        "    def tokenize(texts: List[Tuple[str, str]]):\n",
        "        conversations = [\n",
        "            [\n",
        "                {\"role\": \"user\",\n",
        "                \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\nThe French text: {text}\\n\\nYour English translation:\"\n",
        "                },\n",
        "                {\"role\": \"assistant\",\n",
        "                \"content\": f\"{translation}\"\n",
        "                }\n",
        "            ]\n",
        "            for text, translation in texts\n",
        "        ]\n",
        "\n",
        "        templated = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False)\n",
        "        tokenized = tokenizer(templated, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        # we need to find where the user message ends so we can ignore it for the prob computation. Remember we're only interested in p(translation|source)\n",
        "        prompts_only = [\n",
        "            tokenizer.apply_chat_template(\n",
        "                [conv[0]],\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "            for conv in conversations\n",
        "        ]\n",
        "        prompt_tokenized = tokenizer(prompts_only, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "        prompt_lengths = (prompt_tokenized.input_ids != tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "        # Now mask the source sentence (user message), keep assistant response (translation)\n",
        "        labels = tokenized.input_ids.clone()\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        for i, prompt_len in enumerate(prompt_lengths):\n",
        "            labels[i, :prompt_len] = -100\n",
        "\n",
        "        tokenized[\"labels\"] = labels\n",
        "        return tokenized\n",
        "\n",
        "    for start in range(0, len(texts), batch_size):\n",
        "        batch = texts[start:start+batch_size]\n",
        "        tokenized = tokenize(batch).to(model.device)\n",
        "        context_vectors = model(**tokenized, output_hidden_states=True).hidden_states[-1]\n",
        "        logits = generator.logits(context_vectors)\n",
        "        logits = logits[:, :-1, :].float()\n",
        "        labels = tokenized.labels[:, 1:].long().contiguous()\n",
        "\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        b, s, *_ = log_probs.shape\n",
        "        nll_loss = F.nll_loss(\n",
        "            log_probs.view(-1, logits.shape[-1]),\n",
        "            labels.view(-1),\n",
        "            ignore_index=-100,\n",
        "            reduction=\"none\"\n",
        "        ).view(b, s)\n",
        "\n",
        "        valid_mask = (labels != -100).float()\n",
        "        per_sample_nll = (nll_loss * valid_mask).sum(dim=1) / valid_mask.sum(dim=1)\n",
        "        loglikelihoods.extend((-per_sample_nll).tolist())\n",
        "\n",
        "    return torch.tensor(loglikelihoods)"
      ],
      "metadata": {
        "id": "K-GuPIDPoI5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature_loglihood = {x: log_likelihood(list(zip(data[\"fr\"][\"dev\"], temperature_translated[x]))).mean().item() for x in temperature_translated}"
      ],
      "metadata": {
        "id": "thZgeUCpGvxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature_loglihood"
      ],
      "metadata": {
        "id": "L_6Moepdp3jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = list(zip(*temperature_loglihood.items()))"
      ],
      "metadata": {
        "id": "AF57muofG4p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "eLlRojSPoFWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(\n",
        "    x = x,\n",
        "    y = y,\n",
        "    markers=True, dashes=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "M-tXPc1NoHe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use sacrebleu to evaluate the translations on BLEU and CHRF metrics: https://github.com/mjpost/sacrebleu"
      ],
      "metadata": {
        "id": "0qXNx_4PKBtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "dE9ekUPSoiD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "mnh4CmCYo3Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = temperature_translated[0.1]\n",
        "references = [data[\"fr\"][\"dev\"]]\n",
        "metric.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "nEqe46MWooZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yxgM28X-pn55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}