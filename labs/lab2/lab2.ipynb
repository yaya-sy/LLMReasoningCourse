{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMCDIxFVbucwCnyx5g/RRAL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/LLMReasoningCourse/blob/main/labs/lab2/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Improving the output quality of LLMs with repeated sampling"
      ],
      "metadata": {
        "id": "aDh211g8KHFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see how to improve LLMs for French to English translation by scaling the translation candidates and using a reward score to pick the best translation."
      ],
      "metadata": {
        "id": "XzYXSkYNKSny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.20.0"
      ],
      "metadata": {
        "id": "kSfigSCPa6fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "OmiLVEc9LUWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"haoranxu/X-ALMA-Preference\", split=\"train\")\n",
        "# get only 100 sentences for evaluation\n",
        "raw_datasets = raw_datasets.filter(lambda x: x[\"directions\"] == \"fr-en\")\n",
        "subset = raw_datasets.select(range(100))"
      ],
      "metadata": {
        "id": "0MaSihwFa_Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use `HuggingFaceTB/SmolLM2-360-Instruct` for this lab."
      ],
      "metadata": {
        "id": "hJ4kfHcSLZD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and the tokenizer. Load the model on the GPU if available\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", dtype=torch.bfloat16, token=\"\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Gr43GCRyLVkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Self-Confidence"
      ],
      "metadata": {
        "id": "qCZZvodaLbwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will use self-confidence as reward."
      ],
      "metadata": {
        "id": "a8kanIC_NDDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu evaluate"
      ],
      "metadata": {
        "id": "SdSfiIIQMNUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "5omrIbfCMLrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "from typing import Tuple, List\n",
        "\n",
        "@torch.no_grad()\n",
        "def log_likelihood(texts: List[Tuple[str, str]], batch_size: int=16):\n",
        "    loglikelihoods = []\n",
        "\n",
        "    def tokenize(texts: List[Tuple[str, str]]):\n",
        "        conversations = [\n",
        "            [\n",
        "                {\"role\": \"user\",\n",
        "                \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\nThe French text: {text}\"\n",
        "                },\n",
        "                {\"role\": \"assistant\",\n",
        "                \"content\": f\"{translation}\"\n",
        "                }\n",
        "            ]\n",
        "            for text, translation in texts\n",
        "        ]\n",
        "\n",
        "        templated = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False)\n",
        "        tokenized = tokenizer(templated, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "\n",
        "        # we need to find where the user message ends so we can ignore it for the prob computation. Remember we're only interested in p(translation|source)\n",
        "        prompts_only = [\n",
        "            tokenizer.apply_chat_template(\n",
        "                [conv[0]],\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "            for conv in conversations\n",
        "        ]\n",
        "        prompt_tokenized = tokenizer(prompts_only, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "        prompt_lengths = (prompt_tokenized.input_ids != tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "        # Now mask the source sentence (user message), keep assistant response (translation)\n",
        "        labels = tokenized.input_ids.clone()\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        for i, prompt_len in enumerate(prompt_lengths):\n",
        "            labels[i, :prompt_len] = -100\n",
        "\n",
        "        tokenized[\"labels\"] = labels\n",
        "        return tokenized\n",
        "\n",
        "    for start in range(0, len(texts), batch_size):\n",
        "        batch = texts[start:start+batch_size]\n",
        "        tokenized = tokenize(batch).to(model.device)\n",
        "        labels = tokenized.labels[:, 1:].long().contiguous()\n",
        "        del tokenized[\"labels\"]\n",
        "        logits = model(**tokenized, output_hidden_states=True).logits\n",
        "        logits = logits[:, :-1, :].float()\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        b, s, *_ = log_probs.shape\n",
        "        nll_loss = F.nll_loss(\n",
        "            log_probs.view(-1, logits.shape[-1]),\n",
        "            labels.view(-1),\n",
        "            ignore_index=-100,\n",
        "            reduction=\"none\"\n",
        "        ).view(b, s)\n",
        "\n",
        "        valid_mask = (labels != -100).float()\n",
        "        per_sample_nll = (nll_loss * valid_mask).sum(dim=1) / valid_mask.sum(dim=1)\n",
        "        loglikelihoods.extend((-per_sample_nll).tolist())\n",
        "\n",
        "    return torch.tensor(loglikelihoods)"
      ],
      "metadata": {
        "id": "x8pQ9u6HMilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def tokenize(texts: List[str]):\n",
        "    \"\"\"Tokenize the texts\"\"\"\n",
        "    conversations = [\n",
        "        [{\"role\": \"user\",\n",
        "          \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\nThe French text: {text}\"}]\n",
        "        for text in texts]\n",
        "    # TODO: call apply_chat_template from the tokenizer class with the right arguments to output torch tensors of the token ids\n",
        "    # Which padding side to use? why?\n",
        "    templated = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=True)\n",
        "    tokenized = tokenizer(templated, padding_side=\"left\", padding=True, return_tensors=\"pt\")\n",
        "    return tokenized\n",
        "\n",
        "@torch.no_grad()\n",
        "def rollouts(corpus: List[str], batch_size: int=4, num_samples=4):\n",
        "    \"\"\"For each example in the batch, generate `num_samples` translation.\"\"\"\n",
        "    data = sorted(enumerate(corpus), key=lambda x: tokenize([x[1]]).input_ids.shape[-1])\n",
        "\n",
        "    results = [None] * len(corpus)\n",
        "\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        indices, texts = zip(*data[i : i + batch_size])\n",
        "\n",
        "        tokenized = tokenize(texts)\n",
        "        input_ids = tokenized.input_ids.to(model.device)\n",
        "        b, s = input_ids.shape\n",
        "        input_ids = input_ids.repeat_interleave(num_samples, dim=0)\n",
        "        attention_mask = tokenized.attention_mask.to(model.device)\n",
        "        attention_mask = attention_mask.repeat_interleave(num_samples, dim=0)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            temperature=1.0,\n",
        "            top_p=0.4,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=True\n",
        "        )\n",
        "        outputs = outputs.view(b, num_samples, -1)\n",
        "\n",
        "        for idx, predicted_ids in zip(indices, outputs):\n",
        "            generated_tokens = predicted_ids[:, s:]\n",
        "            translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            print(corpus[idx], \"@@@\", translations)\n",
        "            print(\"---\")\n",
        "            results[idx] = translations\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "X8U4zh2VM2vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = rollouts(french_texts)"
      ],
      "metadata": {
        "id": "MPlsfrf2QxeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Likelihood self-confidence"
      ],
      "metadata": {
        "id": "l-t51w05fHEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Translate the French corpus with num_samples `[1, 2, 4, 8, 16]`\n",
        "2. Compute the log-likelihood of the translation using the LLM itsself\n",
        "3. Pick the most likely translation for each example\n",
        "4. Compute the bleu score, and observe the results"
      ],
      "metadata": {
        "id": "2I_dexJxeIxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entropy self-confidence"
      ],
      "metadata": {
        "id": "orZucyhTfMNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the function `log_likelihood` to compute the entropy. Repeat the previous experiment but with entropy."
      ],
      "metadata": {
        "id": "Yg3K-LOae7gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward modeling"
      ],
      "metadata": {
        "id": "YJf83wgOfari"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train a reward model for translation quality scoring.\n",
        "\n",
        "## Data\n",
        "\n",
        "1. Get 1000 translation pairs from the dataset.\n",
        "2. Translate the French texts to English with repeated sampling (num_samples=4)\n",
        "3. Use the gold English texts as prefered translations and the translations of the model as rejected translations"
      ],
      "metadata": {
        "id": "cBnINYFsfzH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "We will use the Bradley-Terry model:\n",
        "\n",
        "$$\n",
        "p(y_c > y_r|x) = \\frac{\\exp(\\mathbf{c}_p \\cdot \\mathbf{w}^T)}{\\exp(\\mathbf{c}_p \\cdot \\mathbf{w}^T) + \\exp(\\mathbf{c}_r \\cdot \\mathbf{w}^T)}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\mathbf{c}_p = \\text{Transformer}(\\mathbf{x}, y_p)$\n",
        "- $\\mathbf{c}_r = \\text{Transformer}(\\mathbf{x}, y_r)$"
      ],
      "metadata": {
        "id": "JpydOpu5gf9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Derive the formula to get logistic regression model\n",
        "2. Complete the class to implement a reward model\n",
        "3. Train the reward model"
      ],
      "metadata": {
        "id": "48Zgy08bhZGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transformer = ...\n",
        "        self.regression_head = ...\n",
        "\n",
        "    def loss_fn(prefered, rejected):\n",
        "        loss = -(F.logsigmoid(...))\n",
        "        return loss\n",
        "\n",
        "    def forward(input_ids, attention_mask):\n",
        "        hidden_states = ...\n",
        "        logits = ...\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "evBh93taQ2Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(reward_model, prompts, prefered, rejected, lr=0.001):\n",
        "    \"\"\" Training loop.\"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "3r8CWPLuizdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use this reward model to pick the best translations, and evaluate with bleu."
      ],
      "metadata": {
        "id": "Uds1LVtQjHdW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8aM5_XDjLxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}