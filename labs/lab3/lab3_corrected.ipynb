{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQOF97N01oS1rwi4eow4kg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/LLMReasoningCourse/blob/main/labs/lab3/lab3_corrected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you will implement RLOO for translation (but also for other tasks, if you wish) using reinforcement learning.,\n",
        "RLOO is a simple reinforcement learning algorithm for LLMs proposed in 2024. It is based on REINFORCE (1992),\n",
        "You will read the paper (https://aclanthology.org/2024.acl-long.662.pdf) and try to understand why the authors propose going back to such an old and simple algorithm, instead of using a much more recent one like PPO (2017).,\n",
        "\n",
        "1. Recall the pseudo code of REINFORCE\n",
        "2. PPO introduces two important methods: (1) importance ratio clipping and (2) low variance reward estimation. Why the authors of RLOO paper argue that clipping is not necessary in LLMs?\n",
        "4. The authors argue that the way PPO computes the advantages might not be worth it. Why?\n",
        "5. How advantages are computed in RLOO? What is the difference between how GRPO computes the advantages?,\n",
        "5. Is RLOO on-policy or off-policy RL algorithm?"
      ],
      "metadata": {
        "id": "7_HvbzU3OI6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext-numpy2-wheel"
      ],
      "metadata": {
        "id": "4onvKj6yObYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cisnlp/GlotLID.git glotid"
      ],
      "metadata": {
        "id": "Dr2_H33ZQ2Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedModel, Trainer, PreTrainedTokenizer\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict, Callable, Optional, Any\n",
        "import torch.nn.functional as F\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "pCxxWDYaJmXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "from glotid.assets.inference.customlid import CustomLID\n",
        "# download model and get the model path\n",
        "# cache_dir is the path to the folder where the downloaded model will be stored/cached.\n",
        "model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
        "print(\"model path:\", model_path)\n",
        "\n",
        "# load the model\n",
        "limited_languages = ['__label__eng_Latn']\n",
        "lid_model = CustomLID(model_path, languages = limited_languages, mode='after')"
      ],
      "metadata": {
        "id": "ladk3fK7OXAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and the tokenizer. Load the model on the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load student model (will be wrapped with LoRA)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\", dtype=torch.bfloat16, token=\"\")\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
        "\n",
        "# Configure LoRA for the student model\n",
        "# lora_config = LoraConfig(\n",
        "#    r=8,  # LoRA rank\n",
        "#    lora_alpha=16,  # LoRA scaling factor\n",
        "#    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "#    lora_dropout=0.1,\n",
        "#    bias=\"none\",\n",
        "#    task_type=\"CAUSAL_LM\"\n",
        "# )\n",
        "\n",
        "# Wrap the student model with LoRA\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# model.print_trainable_parameters()  # This will show how many parameters are trainable\n",
        "\n",
        "# Load teacher model (no LoRA, keep frozen)\n",
        "teacher = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\", dtype=torch.bfloat16, token=\"\")\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "\n",
        "# Move models to device\n",
        "model = model.to(device)\n",
        "teacher = teacher.to(device)"
      ],
      "metadata": {
        "id": "9do57RWxXR6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"haoranxu/X-ALMA-Preference\", split=\"train\")\n",
        "# get only 100 sentences for evaluation\n",
        "raw_datasets = raw_datasets.filter(lambda x: x[\"directions\"] == \"fr-en\")\n",
        "N = 1000\n",
        "subset = raw_datasets.select(range(N))\n",
        "# use 'chosen' as gold English translations and 'source' as French sentences to translate to English"
      ],
      "metadata": {
        "id": "XXj36Mj41qH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templated = subset.map(lambda x: {\"conversations\": [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\n{x['source']}\"\n",
        "     }]})\n",
        "templated = templated.map(lambda x: {\"templated\": student_tokenizer.apply_chat_template(x[\"conversations\"], tokenize=False, add_generation_prompt=True)})\n",
        "print(templated[0])\n",
        "tokenized_training_data = templated.map(lambda x: student_tokenizer(x[\"templated\"], return_tensors=\"pt\"))\n",
        "tokenized_training_data = tokenized_training_data.map(lambda x: {\"source_input_ids\": student_tokenizer(x['source']).input_ids})\n",
        "tokenized_training_data = tokenized_training_data.remove_columns([\"conversations\", \"templated\"])\n",
        "tokenized_training_data.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"source_input_ids\"])"
      ],
      "metadata": {
        "id": "qUU1U6GZdLbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RewardDataCollatorWithPadding:\n",
        "    tokenizer: AutoTokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        features_conv = [{\"input_ids\": f[\"input_ids\"].squeeze(), \"attention_mask\": f[\"attention_mask\"].squeeze()} for f in features]\n",
        "        features_source = [{\"input_ids\": f[\"source_input_ids\"].squeeze()} for f in features]\n",
        "\n",
        "        batch_conv = self.tokenizer.pad(\n",
        "            features_conv,\n",
        "            padding=True,\n",
        "            padding_side=\"left\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        batch_source = self.tokenizer.pad(\n",
        "            features_source,\n",
        "            padding=True,\n",
        "            padding_side=\"left\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": batch_conv[\"input_ids\"],\n",
        "            \"attention_mask\": batch_conv[\"attention_mask\"],\n",
        "            \"source_input_ids\": batch_source[\"input_ids\"],\n",
        "            \"source_attention_mask\": batch_source[\"attention_mask\"],\n",
        "        }"
      ],
      "metadata": {
        "id": "zxtHOPLqfdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_user_and_assistant(tokenizer, texts: List[str]):\n",
        "      conversations = [\n",
        "          [\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\n{text}\"\n",
        "              },\n",
        "              {\"role\": \"assistant\",\n",
        "              \"content\": f\"{translation}\"\n",
        "              }\n",
        "          ]\n",
        "          for text, translation in texts\n",
        "      ]\n",
        "\n",
        "      templated = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False, enable_thinking=False)\n",
        "      tokenized = tokenizer(templated, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "\n",
        "      # Loss is computed only on the assistant answers\n",
        "      prompts_only = [\n",
        "          tokenizer.apply_chat_template(\n",
        "              [conv[0]],\n",
        "              tokenize=False,\n",
        "              add_generation_prompt=True\n",
        "          )\n",
        "          for conv in conversations\n",
        "      ]\n",
        "      prompt_tokenized = tokenizer(prompts_only, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "      prompt_lengths = (prompt_tokenized.input_ids != tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "      # Now mask the source sentence (user message), keep assistant response (translation)\n",
        "      labels = tokenized.input_ids.clone()\n",
        "      labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "      for i, prompt_len in enumerate(prompt_lengths):\n",
        "          labels[i, :prompt_len] = -100\n",
        "\n",
        "      tokenized[\"labels\"] = labels\n",
        "      return tokenized"
      ],
      "metadata": {
        "id": "NqAI4QJYXiS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHNgraB7wxqL"
      },
      "outputs": [],
      "source": [
        "class RLOOTrainer(Trainer):\n",
        "    def __init__(self, model, teacher, student_tokenizer, teacher_tokenizer, num_samples: int = 4, batch_size: int = 4, *args, **kwargs):\n",
        "        super(RLOOTrainer, self).__init__(model=model, *args, **kwargs)\n",
        "        self.student_tokenizer = student_tokenizer\n",
        "        self.teacher_tokenizer = teacher_tokenizer\n",
        "        self.teacher = teacher\n",
        "        for p in self.teacher.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "        self.local_steps = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def rollouts(self, model, inputs):\n",
        "        b, s = inputs[\"input_ids\"].shape\n",
        "        input_ids = inputs[\"input_ids\"].repeat_interleave(self.num_samples, dim=0)\n",
        "        attention_mask = inputs[\"attention_mask\"].repeat_interleave(self.num_samples, dim=0)\n",
        "\n",
        "        all_translations = []\n",
        "        total_samples = b * self.num_samples\n",
        "\n",
        "        for start_idx in range(0, total_samples, self.batch_size):\n",
        "            end_idx = min(start_idx + self.batch_size, total_samples)\n",
        "            batch_input_ids = input_ids[start_idx:end_idx]\n",
        "            batch_attention_mask = attention_mask[start_idx:end_idx]\n",
        "\n",
        "            outputs = model.generate(\n",
        "                input_ids=batch_input_ids,\n",
        "                attention_mask=batch_attention_mask,\n",
        "                temperature=1.0,\n",
        "                top_p=0.9,\n",
        "                max_new_tokens=128,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "            generated_ids = outputs[:, s:].cpu()\n",
        "            translations = self.student_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            # print(translations)\n",
        "            all_translations.extend(translations)\n",
        "\n",
        "        grouped_translations = [\n",
        "            all_translations[i:i + self.num_samples]\n",
        "            for i in range(0, len(all_translations), self.num_samples)\n",
        "        ]\n",
        "        return grouped_translations\n",
        "\n",
        "    def _compute_token_logps(self, model, model_inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        all_logps = []\n",
        "        total_samples = model_inputs[\"input_ids\"].size(0)\n",
        "\n",
        "        for start_idx in range(0, total_samples, self.batch_size):\n",
        "            end_idx = min(start_idx + self.batch_size, total_samples)\n",
        "            batch_inputs = {\n",
        "                k: v[start_idx:end_idx] for k, v in model_inputs.items()\n",
        "            }\n",
        "\n",
        "            logits = model(\n",
        "                input_ids=batch_inputs[\"input_ids\"],\n",
        "                attention_mask=batch_inputs[\"attention_mask\"]\n",
        "            ).logits\n",
        "\n",
        "            shifted_logits = logits[:, :-1, :].float().contiguous()\n",
        "            shifted_labels = batch_inputs[\"labels\"][:, 1:].long().contiguous()\n",
        "            b, s, vocab_size = shifted_logits.shape\n",
        "\n",
        "            nll = F.cross_entropy(\n",
        "                shifted_logits.view(-1, vocab_size),\n",
        "                shifted_labels.view(-1),\n",
        "                ignore_index=-100,\n",
        "                reduction=\"none\"\n",
        "            ).view(b, s)\n",
        "\n",
        "            all_logps.append(-nll.cpu())\n",
        "\n",
        "        return torch.cat(all_logps, dim=0)\n",
        "\n",
        "    def training_step(self, model, inputs, *args, **kwargs):\n",
        "        if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n",
        "            self.optimizer.train()\n",
        "\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        student_tokenized, student_mask, advantages, rewards = self.compute_loss(model, inputs)\n",
        "        losses = 0.0\n",
        "        total = 0.0\n",
        "        model.train()\n",
        "        total_samples = student_tokenized[\"input_ids\"].shape[0]\n",
        "        for start_idx in range(0, total_samples, self.batch_size):\n",
        "            end_idx = min(start_idx + self.batch_size, total_samples)\n",
        "            batch_inputs = {\n",
        "                k: v[start_idx:end_idx] for k, v in student_tokenized.items()\n",
        "            }\n",
        "            batch_advantages = advantages[start_idx:end_idx]\n",
        "            student_token_logprobs = self._compute_token_logps(model=model, model_inputs=batch_inputs)\n",
        "            batch_mask = student_mask[start_idx:end_idx]\n",
        "            student_seq_logprobs = (student_token_logprobs * batch_mask).sum(-1)\n",
        "            loss = -(batch_advantages * student_seq_logprobs).mean()\n",
        "            self.accelerator.backward(loss, **kwargs)\n",
        "            loss.detach()\n",
        "            losses += loss\n",
        "            total += 1\n",
        "        self.local_steps += 1\n",
        "        if self.local_steps % 5 == 0:\n",
        "            print(\n",
        "                {'train/mean_advantages': advantages.mean().item(),\n",
        "                 'train/mean_rewards': rewards.mean().item(),\n",
        "                 'train/std_advantages': advantages.std().item(),\n",
        "                 'train/max_reward': rewards.max().item(),\n",
        "                 'train/min_reward': rewards.min().item()\n",
        "                 })\n",
        "\n",
        "        return (losses / total).detach().to(model.device)\n",
        "\n",
        "    def compute_loss(self, model, inputs, *args, **kwargs):\n",
        "        model.eval()\n",
        "        translated = list(itertools.chain.from_iterable(self.rollouts(model, inputs)))\n",
        "        lengths = (inputs[\"source_attention_mask\"] == 0).sum(-1)\n",
        "        source_input_ids = []\n",
        "        for idx, length in enumerate(lengths):\n",
        "            source_input_ids.append(self.student_tokenizer.decode(inputs[\"source_input_ids\"][idx, length:].tolist(), skip_special_tokens=True))\n",
        "        source = list(itertools.chain.from_iterable([[s] * self.num_samples for s in source_input_ids]))\n",
        "        translated = [t.replace('system\\n', '').replace('assistant\\n', '') for t in translated]\n",
        "        assert len(source) == len(translated)\n",
        "        source_translated = list(zip(source, translated))\n",
        "\n",
        "        student_tokenized = tokenize_user_and_assistant(self.student_tokenizer, source_translated)\n",
        "        student_tokenized = {k: v.to(model.device) for k, v in student_tokenized.items()}\n",
        "        student_mask = (student_tokenized[\"labels\"] != -100).float()[:, 1:].cpu()\n",
        "\n",
        "        teacher_tokenized = tokenize_user_and_assistant(self.teacher_tokenizer, source_translated)\n",
        "        teacher_tokenized = {k: v.to(model.device) for k, v in teacher_tokenized.items()}\n",
        "        teacher_mask = (teacher_tokenized[\"labels\"] != -100).float()[:, 1:].cpu()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            lid_rewards = torch.tensor([lid_model.predict(translation.replace(\"\\n\", \"\"))[-1].item() for translation in translated]).log()\n",
        "            teacher_token_logprobs = self._compute_token_logps(model=self.teacher, model_inputs=teacher_tokenized)\n",
        "            teacher_seq_logprob = (teacher_token_logprobs * teacher_mask).sum(-1) / teacher_mask.sum(-1)\n",
        "\n",
        "            src_lens = torch.tensor([len(s.split()) for s in source], device=model.device, dtype=torch.float)\n",
        "            pred_lens = torch.tensor([len(t.split()) for t in translated], device=model.device, dtype=torch.float)\n",
        "            len_ratio = torch.abs(src_lens - pred_lens)\n",
        "            length_rewards = -torch.log(len_ratio + 1e-12).to(teacher_seq_logprob.device)\n",
        "\n",
        "            rewards = lid_rewards + teacher_seq_logprob + length_rewards\n",
        "\n",
        "            grouped_rewards = rewards.view(-1, self.num_samples)\n",
        "            grouped_sum = grouped_rewards.sum(dim=-1, keepdim=True)\n",
        "            baselines = (grouped_sum - grouped_rewards) / (self.num_samples - 1)\n",
        "            baselines = baselines.view(-1)\n",
        "            advantages = rewards - baselines\n",
        "\n",
        "        return student_tokenized, student_mask, advantages.detach(), rewards.detach()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the trainer\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"rm\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=400,\n",
        "    gradient_accumulation_steps=1,\n",
        "    remove_unused_columns=False,\n",
        "    bf16=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=5,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.03,\n",
        "    report_to='none',\n",
        ")"
      ],
      "metadata": {
        "id": "UfEAlqzGDtCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = RLOOTrainer(\n",
        "    model=model,\n",
        "    teacher=teacher,\n",
        "    student_tokenizer=student_tokenizer,\n",
        "    teacher_tokenizer=teacher_tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_training_data,\n",
        "    data_collator=RewardDataCollatorWithPadding(tokenizer=student_tokenizer),\n",
        ")"
      ],
      "metadata": {
        "id": "BiveRkA_plmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9tt_j0kFqzwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = raw_datasets.select(range(9000, 9200))"
      ],
      "metadata": {
        "id": "Q6iZ-Mzaic3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\n{test[87]['source']}\"}]\n",
        "input_text = student_tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = student_tokenizer.encode(input_text, return_tensors=\"pt\").to(trainer.model.device)\n",
        "outputs = trainer.model.generate(inputs, max_new_tokens=128, temperature=0.2, top_p=0.9, do_sample=True)\n",
        "print(student_tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "8ij_80_4q1km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\", dtype=torch.bfloat16, token=\"\")"
      ],
      "metadata": {
        "id": "UrB1IzqT3UPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = base.cuda()"
      ],
      "metadata": {
        "id": "JeoP-rmRcBli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\n{test[87]['source']}\"}]\n",
        "input_text = student_tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(input_text)\n",
        "inputs = student_tokenizer.encode(input_text, return_tensors=\"pt\").to(trainer.model.device)\n",
        "outputs = base.generate(inputs, max_new_tokens=128, temperature=0.2, top_p=0.9, do_sample=True)\n",
        "print(student_tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "3shddbu6FdUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "@torch.no_grad()\n",
        "def translate(corpus: List[str], model=None, tokenizer=None, batch_size: int=4, num_samples=1):\n",
        "    \"\"\"For each example in the batch, generate `num_samples` translations.\"\"\"\n",
        "    def tokenize(texts: List[str]):\n",
        "        \"\"\"Tokenize the texts\"\"\"\n",
        "        conversations = [\n",
        "            [{\"role\": \"user\",\n",
        "              \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\n{text}\"}]\n",
        "            for text in texts]\n",
        "        # TODO: call apply_chat_template from the tokenizer class with the right arguments to output torch tensors of the token ids\n",
        "        # Which padding side to use? why?\n",
        "        templated = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=True)\n",
        "        tokenized = tokenizer(templated, padding_side=\"left\", padding=True, return_tensors=\"pt\")\n",
        "        return tokenized\n",
        "    data = sorted(enumerate(corpus), key=lambda x: tokenize([x[1]]).input_ids.shape[-1])\n",
        "\n",
        "    results = [None] * len(corpus)\n",
        "\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        indices, texts = zip(*data[i : i + batch_size])\n",
        "\n",
        "        tokenized = tokenize(texts)\n",
        "        input_ids = tokenized.input_ids.to(model.device)\n",
        "        b, s = input_ids.shape\n",
        "        input_ids = input_ids.repeat_interleave(num_samples, dim=0)\n",
        "        attention_mask = tokenized.attention_mask.to(model.device)\n",
        "        attention_mask = attention_mask.repeat_interleave(num_samples, dim=0)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            temperature=0.2,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=True\n",
        "        )\n",
        "        outputs = outputs.view(b, num_samples, -1)\n",
        "\n",
        "        for idx, predicted_ids in zip(indices, outputs):\n",
        "            generated_tokens = predicted_ids[:, s:]\n",
        "            translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            results[idx] = translations\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "faqPg93YH0q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_base = translate(test[\"source\"], model=base, tokenizer=student_tokenizer)"
      ],
      "metadata": {
        "id": "iCe9nCl-oTd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_student = translate(test[\"source\"], model=trainer.model, tokenizer=student_tokenizer)"
      ],
      "metadata": {
        "id": "EKFx_wzldlra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu evaluate"
      ],
      "metadata": {
        "id": "7KZxYhWFhYZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "FyAWwdVAh5bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "g_bLapeOhaCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_candidates = [t[0] for t in translated_base]\n",
        "student_candidates = [t[0] for t in translated_student]\n",
        "\n",
        "references = [[t] for t in test[\"chosen\"]]\n",
        "print(\"base:\", metric.compute(predictions=base_candidates, references=references)[\"score\"])\n",
        "print(\"rloo:\", metric.compute(predictions=student_candidates, references=references)[\"score\"])"
      ],
      "metadata": {
        "id": "jBFizcyZgkGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_candidates[0], student_candidates[0], references[0]"
      ],
      "metadata": {
        "id": "-weFZ6E1iIRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1N_9RoKAidvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}