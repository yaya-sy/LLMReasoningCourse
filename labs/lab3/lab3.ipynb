{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMXPS4VEysLNKlfsyg2Cyo/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/LLMReasoningCourse/blob/main/labs/lab3/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: RLOO Implementation"
      ],
      "metadata": {
        "id": "A4yfYhwxNltm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you will implement RLOO for translation (but also for other tasks, if you wish) using reinforcement learning.\n",
        "RLOO is a simple reinforcement learning algorithm for LLMs proposed in 2024. It is based on REINFORCE (1992).\n",
        "You will read the paper (https://aclanthology.org/2024.acl-long.662.pdf\n",
        ") and try to understand why the authors propose going back to such an old and simple algorithm, instead of using a much more recent one like PPO (2017).\n",
        "\n",
        "1. Recall the pseudo code of REINFORCE\n",
        "\n",
        "\n",
        "PPO introduces two important methods: (1) importance ratio clipping and (2) low variance reward estimation.\n",
        "\n",
        "2. Why the authors of RLOO paper argue that clipping is not necessary in LLMs?\n",
        "3. The authors argue that the way PPO computes the advantages might not be worth it. Why?\n",
        "4. How advantages are computed in RLOO? What is the difference between how GRPO computes the advantages?\n",
        "5. Is RLOO on-policy or off-policy RL algorithm?"
      ],
      "metadata": {
        "id": "G8Dl5NKlNpO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "pCxxWDYaJmXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and the tokenizer. Load the model on the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = # Use lora if you don't have enough memory or want to scale the model size\n",
        "reward_model  = # Use a reward model if needed."
      ],
      "metadata": {
        "id": "9do57RWxXR6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"haoranxu/X-ALMA-Preference\", split=\"train\")\n",
        "raw_datasets = raw_datasets.filter(lambda x: x[\"directions\"] == \"fr-en\")\n",
        "N = 1000\n",
        "subset = raw_datasets.select(range(N))\n",
        "# use 'chosen' as gold English translations and 'source' as French sentences to translate to English"
      ],
      "metadata": {
        "id": "XXj36Mj41qH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare and tokenize the dataset"
      ],
      "metadata": {
        "id": "qUU1U6GZdLbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "    tokenizer: AutoTokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        features_conv = [{\"input_ids\": f[\"input_ids\"].squeeze(), \"attention_mask\": f[\"attention_mask\"].squeeze()} for f in features]\n",
        "        features_source = [{\"input_ids\": f[\"source_input_ids\"].squeeze()} for f in features]\n",
        "\n",
        "        batch_conv = self.tokenizer.pad(\n",
        "            features_conv,\n",
        "            padding=True,\n",
        "            padding_side=\"left\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        batch_source = self.tokenizer.pad(\n",
        "            features_source,\n",
        "            padding=True,\n",
        "            padding_side=\"left\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": batch_conv[\"input_ids\"],\n",
        "            \"attention_mask\": batch_conv[\"attention_mask\"],\n",
        "            \"source_input_ids\": batch_source[\"input_ids\"],\n",
        "            \"source_attention_mask\": batch_source[\"attention_mask\"],\n",
        "        }"
      ],
      "metadata": {
        "id": "zxtHOPLqfdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_user_and_assistant(tokenizer, texts: List[str]):\n",
        "      conversations = [\n",
        "          [\n",
        "              {\"role\": \"user\",\n",
        "              \"content\": f\"You are given a French text, provide faithful translation to English.\\n\\n{text}\"\n",
        "              },\n",
        "              {\"role\": \"assistant\",\n",
        "              \"content\": f\"{translation}\"\n",
        "              }\n",
        "          ]\n",
        "          for text, translation in texts\n",
        "      ]\n",
        "\n",
        "      templated = tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=False)\n",
        "      tokenized = tokenizer(templated, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "\n",
        "      # Loss is computed only on the assistant answers\n",
        "      prompts_only = [\n",
        "          tokenizer.apply_chat_template(\n",
        "              [conv[0]],\n",
        "              tokenize=False,\n",
        "              add_generation_prompt=True\n",
        "          )\n",
        "          for conv in conversations\n",
        "      ]\n",
        "      prompt_tokenized = tokenizer(prompts_only, padding_side=\"right\", padding=True, return_tensors=\"pt\")\n",
        "      prompt_lengths = (prompt_tokenized.input_ids != tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "      # Now mask the source sentence (user message), keep assistant response (translation)\n",
        "      labels = tokenized.input_ids.clone()\n",
        "      labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "      for i, prompt_len in enumerate(prompt_lengths):\n",
        "          labels[i, :prompt_len] = -100\n",
        "\n",
        "      tokenized[\"labels\"] = labels\n",
        "      return tokenized"
      ],
      "metadata": {
        "id": "NqAI4QJYXiS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHNgraB7wxqL"
      },
      "outputs": [],
      "source": [
        "class RLOOTrainer(Trainer):\n",
        "    def __init__(self, model, reward_model, student_tokenizer, reward_tokenizer, num_samples: int = 8, *args, **kwargs):\n",
        "        super(RLOOTrainer, self).__init__(model=model, *args, **kwargs)\n",
        "        self.student_tokenizer = student_tokenizer\n",
        "        self.reward_tokenizer = reward_tokenizer\n",
        "        self.reward_model = reward_model\n",
        "        self.reward_model.eval()\n",
        "        for p in self.reward_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def rollouts(self, model, inputs):\n",
        "        # will generate translations from the model, given the tokenized inputs\n",
        "        return translations\n",
        "\n",
        "    def _compute_token_logps(self, model, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        # computes the logprobs on the inputs under the model\n",
        "\n",
        "    def compute_loss(self, model, inputs, *args, **kwargs):\n",
        "        model.eval()\n",
        "        translated = list(itertools.chain.from_iterable(self.rollouts(model, inputs)))\n",
        "        lengths = (inputs[\"source_attention_mask\"] == 0).sum(-1)\n",
        "        source_input_ids = []\n",
        "        for idx, length in enumerate(lengths):\n",
        "            source_input_ids.append(self.student_tokenizer.decode(inputs[\"source_input_ids\"][idx, length:].tolist(), skip_special_tokens=True))\n",
        "        source = list(itertools.chain.from_iterable([[s] * self.num_samples for s in source_input_ids]))\n",
        "        # remove 'system\\n' and 'assistant\\n\n",
        "        translated = [t.replace('system\\n', '').replace('assistant\\n', '') for t in translated]\n",
        "        assert len(source) == len(translated)\n",
        "        source_translated = list(zip(source, translated))\n",
        "\n",
        "        tokenized = tokenize_user_and_assistant(self.student_tokenizer, source_translated)\n",
        "        tokenized = {k: v.to(model.device) for k, v in student_tokenized.items()}\n",
        "        mask = (student_tokenized[\"labels\"] != -100).float()\n",
        "        mask = student_mask[:, 1:]\n",
        "\n",
        "        reward_tokenized = \"...\"\n",
        "        reward_mask = \"...\" # if needed\n",
        "        reward_mask = teacher_mask[:, 1:]\n",
        "\n",
        "        policy_token_logprobs = self._compute_token_logps(model=model, model_inputs=tokenized)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # compute the rewards and advantages\n",
        "\n",
        "        model.train()\n",
        "        student_seq_logprobs = (policy_token_logprobs * mask).sum(-1)\n",
        "        loss = -(advantages.detach() * policy_token_logprobs).mean()\n",
        "        if self.state.global_step % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                metrics = {\n",
        "                    \"train/mean_reward\": rewards.mean().item(),\n",
        "                    \"train/mean_student_logprob\": student_seq_logprobs.mean().item(),\n",
        "                }\n",
        "                print(metrics)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're working on Translation Reinforcement Learning, you have different choices of reward. You can manually your own reward function (very difficult!). Maybe you can also use translation evaluation models:\n",
        "- BERT-Score\n",
        "- Reference-Free COMET models\n",
        "- Multilingual Embedding Models"
      ],
      "metadata": {
        "id": "G-oJhe_D-2CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the trainer\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"rm\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=400,\n",
        "    gradient_accumulation_steps=1,\n",
        "    remove_unused_columns=False,\n",
        "    bf16=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    report_to='none',\n",
        ")"
      ],
      "metadata": {
        "id": "UfEAlqzGDtCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = RLOOTrainer(\n",
        "    model=model,\n",
        "    teacher=teacher,\n",
        "    student_tokenizer=student_tokenizer,\n",
        "    teacher_tokenizer=teacher_tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_training_data,\n",
        "    data_collator=RewardDataCollatorWithPadding(tokenizer=student_tokenizer),\n",
        ")"
      ],
      "metadata": {
        "id": "BiveRkA_plmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9tt_j0kFqzwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "faqPg93YH0q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Training agentic poet"
      ],
      "metadata": {
        "id": "3hswlvOwQm6k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C09QuQyTQruT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}