# LLMReasoningCourse

## Course 1
We present the LLM as a probability function estimated on the web using the Maximum Likelihood Estimation. After training, text can be generated by sampling from this distribution token by token. We introduce the main sampling strategies. **Greedy** decoding selects the most probable token at each step. **Top-K** sampling, which samples from the _K_ most probable tokens. Both methods can suffer from low diversity, so we present **temperature** as a hyperparameter to control diversity of generated tokens. Finally, to balance diversity and coherence, we present **Top-P** (nucleus) sampling, which adaptively samples a variable number of the most probable tokens based on their cumulative probability. The lab will be about implementing these methods from scratch.

## Course 2
Sampling a single answer per question can be insufficient to obtain the correct solution. We therefore introduce **repeated sampling**, where the LLM generates multiple candidate answers and the best one is selected. We show that increasing the number of attempts improves performance. Selecting the best candidate requires a **reward function** to evaluate answer quality, and we present approaches for both **verifiable** and **non-verifiable** tasks, including model **self-confidence** and learned neural **reward models**. The lab will be about exploring model self-confidence for translation task and implementing a reward model training based on Bradley-Terry approach.

## Course 3
Repeated Sampling searches for the answer that maximizes a reward, whereas **Reinforcement Learning** (RL) searches for the model that maximizes expected reward. After a brief introduction to reinforcement learning, we explain how it is used to train **reasoning LLMs**. We also discuss LLM RL training for non-verifiable tasks, instabilities, and the challenges of scaling long chain-of-thought reasoning. The lab will be about implementing REINFORCE Leave-One-Out for translation.